{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Hugging Face Transformers\n",
    "\n",
    "In this notebook, we demonstrate how to use a Hugging Face model for text generation.\n",
    "\n",
    "We will use the `transformers` library to:\n",
    "\n",
    "1. Load a pre-trained language model.\n",
    "2. Tokenize input text.\n",
    "3. Generate text based on a given prompt.\n",
    "\n",
    "### Dependencies\n",
    "- `transformers`\n",
    "- `torch`\n",
    "- `tokenizers`\n",
    "\n",
    "To install the dependencies, use:\n",
    "```bash\n",
    "pip install transformers torch tokenizers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries from Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load a Pre-trained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model name (GPT-2 in this case)\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenize Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input prompt for text generation\n",
    "input_prompt = \"Once upon a time, in a land far away,\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors='pt')\n",
    "\n",
    "# Display the tokenized input\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Once upon a time, in a land far away, there lived a young\n",
      "fairy who had a special gift. She could control the elements with\n",
      "a wave of her hand. The villagers marveled at her abilities, and she\n",
      "was known throughout the kingdom as the Elemental Fairy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on the input prompt\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Display the generated text\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to './huggingface_gpt2'.\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer to a directory\n",
    "save_directory = \"./huggingface_gpt2\"\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(f\"Model and tokenizer saved to '{save_directory}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load the Saved Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model and tokenizer from './huggingface_gpt2'.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer from the saved directory\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "print(f\"Loaded model and tokenizer from '{save_directory}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Text with the Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text with loaded model:\n",
      "Once upon a time, in a land far away, there lived a young\n",
      "girl named Sarah. She had a dream to become the best baker\n",
      "in the world. Every day, she practiced making delicious pastries and cakes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate text with the loaded model\n",
    "new_prompt = \"Once upon a time, in a land far away, there lived a young\"\n",
    "new_input_ids = loaded_tokenizer.encode(new_prompt, return_tensors='pt')\n",
    "new_output = loaded_model.generate(new_input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "\n",
    "# Decode the generated text\n",
    "new_generated_text = loaded_tokenizer.decode(new_output[0], skip_special_tokens=True)\n",
    "\n",
    "# Display the generated text\n",
    "print(\"Generated text with loaded model:\")\n",
    "print(new_generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
     "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
